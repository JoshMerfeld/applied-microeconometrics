---
title:  |
    | Microeconometrics (Causal Inference)
    | Weeks 7 and 8 - Instrumental variables
author:
  |
    | Joshua D. Merfeld
    | KDI School of Public Policy and Management
date: "`r Sys.Date()`"

# Output type and options
output: 
  beamer_presentation:
    theme: Montpellier
classoption: "aspectratio=169"

# This includes latex arguments
header-includes:
  - \AtBeginDocument{\title[Weeks 7 and 8 - IV]{Microeconometrics (Causal Inference) \\ Weeks 7 and 8 - Instrumental variables}}
  - \AtBeginDocument{\author[Josh Merfeld - KDI School]{Joshua D. Merfeld \\ KDI School of Public Policy and Management}}
  - \input{header.tex}
  - \usepackage[flushleft]{threeparttable}

---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, dev = "png") # NOTE: switched to png instead of pdf to decrease size of the resulting pdf

def.chunk.hook  <- knitr::knit_hooks$get("chunk")
knitr::knit_hooks$set(chunk = function(x, options) {
  x <- def.chunk.hook(x, options)
  #ifelse(options$size != "a", paste0("\n \\", "tiny","\n\n", x, "\n\n \\normalsize"), x)
  ifelse(options$size != "normalsize", paste0("\n \\", options$size,"\n\n", x, "\n\n \\normalsize"), x)
})




library(tidyverse)
library(kableExtra)
library(fixest)
library(ggpubr)
library(RColorBrewer)
library(haven)
library(fwildclusterboot)
library(modelsummary)



```




## What are we doing today?
\vfill
- Introduction to IVs
  - Requirements/assumptions
\vfill
- IVs and RCTs
\vfill
- In a world of LATE
\vfill
- Weak instruments
\vfill













## Instrumental variables
\vfill
- Instrumental variables (IVs) are a way to estimate causal effects when we have endogeneity
  - The endogeneity can take many forms: omitted variables, measurement error, simultaneity, etc.
\vfill
- Consider my paper: effects of pollution on agricultural productivity
  - What's the problem with simply regression productivity on pollution?
\vfill











## Endogeneity in the pollution example
\vfill
\center
\includegraphics[width = 0.5\textwidth]{assets/pollution1.png}
\vfill











## Endogeneity in the pollution example
\vfill
\center
\includegraphics[width = 0.5\textwidth]{assets/pollution2.png}
\vfill










## Putting structure on this
\vfill
- What we really want to estimate is this:
\begin{gather} \label{eq:iv1} productivity_{it} = \beta_0 + \beta_1 pollution_{it} + \epsilon_{it} \end{gather}
where $\beta_1$ is the causal effect of pollution on productivity.

\vfill
- Endogeneity is defined as $cov(pollution_{it}, \epsilon_{it})\neq0$
  - That is, the error term is correlated with the endogenous variable
  - A common example is omitted variables
\vfill










## Putting structure on this
\vfill
\begin{gather} \tag{1} productivity_{it} = \beta_0^* + \beta_1^* pollution_{it} + \epsilon_{it}^* \end{gather}

- When we estimate this, due to the way OLS works, the residuals and pollution will be orthogonal
  - That is, $cov(pollution_{it}, \epsilon_{it}^*)=0$
  - This is a property of OLS

\vfill
- However, the issue is that under endogeneity, $\beta^*_1\neq\beta_1$
  - That is, the OLS estimate of $\beta_1$ is biased *for the true structural parameter*
\vfill










## Putting structure on this
- Another way to think about it is that what we want to estimate is this:
\begin{gather} productivity_{it} = \beta_0 + \beta_1 pollution_{it} + \beta_2 X + \epsilon_{it} \end{gather}

- But if we don't properly control for everything -- in this case $X$ -- we are really estimating this:
\begin{gather} \label{eq:iv2} productivity_{it} = \tilde{\beta_0} + \tilde{\beta_1} pollution_{it} + \eta_{it}, \end{gather}
where $\eta_{it} = \beta_2 X_{it} + \epsilon_{it}$.

\vfill











## Differences in differences?
\vfill
- One solution is to use a differences-in-differences (DiD) approach
\vfill
- This requires the assumption of parallel trends
  - That is, the trends in the outcome variable would have been the same in the absence of the treatment
\vfill
- But what if changing economic growth is leading to changes in both pollution and productivity?
  - Then the parallel trends assumption is violated since areas with more pollution are also experiencing faster economic growth
\vfill











## Control for growth?
\vfill
- If you're willing to make assumptions about what the omitted variables are, maybe you could control for theme
\vfill
- But this is a strong assumption
  - No matter what we do, we'll have to make assumptions, though
\vfill











## Enter: instruments
\vfill
- Let's take a different approach
\vfill
- We'll use an instrument
  - A variable that is correlated with the endogenous variable (pollution) but is not correlated with the error term
\vfill











## Instrument in the pollution example
\vfill
\center
\includegraphics[width = 0.75\textwidth]{assets/pollution3.png}
\vfill











## Requirements of an instrument
\vfill
- I very purposefully created the example so that the instrument is correlated with pollution
  - But it's not *directly* correlated with productivity
  - And it's not correlated with the omitted variable (the error term... will show you this in a second)
\vfill
- Let's look at these more formally
\vfill










## Back to our problem
\vfill
\begin{gather} \tag{3} productivity_{it} = \tilde{\beta_0} + \tilde{\beta_1} pollution_{it} + \eta_{it} \end{gather}

- Can we estimate a version of this equation -- that is, without controlling for $X_{it}$ -- and still get causal effects?
\vfill
- Maybe, if we can find a valid instrument.
\vfill
- So what makes an instrument valid?
\vfill










## What else can instruments help with?
\vfill
- It turns out IVs can also help with measurement error
  - If we have a variable that is measured with error, we can use an instrument to correct for this
\vfill
- From Hansen, consider the model:
\begin{gather} X = Q + u, \end{gather}
where $X$ is the variable we observe, $Q$ is the variable we want to measure, and $u$ is measurement error.

- Assume that $cov(u, Q)=0$, so that the measurement error is *random*, i.e. uncorrelated with the true value of $Q$.
  - This is known as classical measurement error
\vfill










## Classical measurement error and attenuation bias
\vfill
- We want to estimate:
\begin{gather} Y = \beta_0 + \beta_1 Q + \epsilon, \end{gather}
but what we really estimate is:
\begin{gather} Y = \tilde{\beta}_0 + \tilde{\beta}_1 X + \tilde{\epsilon} = \tilde{\beta}_0 + \tilde{\beta}_1 (Q + u) + \tilde{\epsilon} \end{gather}
\vfill









## Classical measurement error and attenuation bias
\vfill
- This is what we get:
\begin{gather} \tilde{\beta}_1 = \beta_1\left(1-\frac{\mathbb{E}(u^2)}{\mathbb{E}(X^2)}\right) \end{gather}

- By definition, $\mathbb{E}(X^2)>\mathbb{E}(u^2)$, so $\tilde{\beta}_1<\beta_1$.
  - Why is this true? \vfill
  - That is, the OLS estimate of $\beta_1$ is biased *towards zero*
  - This is called attenutation bias, but is only guaranteed with the measurement error is classical (random)
\vfill









## Requirements for an instrument
\vfill
\begin{gather} \tag{3} productivity_{it} = \tilde{\beta_0} + \tilde{\beta_1} pollution_{it} + \eta_{it} \end{gather}

1. The instrument must be correlated with the endogenous variable (pollution)
\vfill
2. The instrument must not be correlated with the error term ($\eta_{it}$)
      - Note that this implies two things:
        - The instrument must not be correlated with any omitted variable (here $X_{it}$)
        - The instrument must not directly affect the outcome ($productivity_{it}$)
\vfill










## Using an instrument
\vfill
- If we can find a valid instrument, we can use it to estimate the causal effect of pollution on productivity
\vfill
- The simplest example uses two stages:
  1. $pollution_{it} = \pi_0 + \pi_1 instrument_{it} + \nu_{it}$
  2. $productivity_{it} = \phi_0 + \phi_1 pollution_{it} + \zeta_{it}$
\vfill
- We can then estimate $\phi_1$ using OLS
  - Note that only under certain circumstances will $\phi_1=\beta_1$
  - More on this later
\vfill










## The intuition with venn diagrams
\center
\includegraphics[width = 0.65\textwidth]{assets/iv1.png}










## The IV only affects productivity through pollution
\center
\includegraphics[width = 0.65\textwidth]{assets/iv2.png}










## This doesn't work. Direct effects on productivity!
\center
\includegraphics[width = 0.65\textwidth]{assets/iv3.png}










## This doesn't work. Correlated with growth!
\center
\includegraphics[width = 0.65\textwidth]{assets/iv4.png}










## Back to our "two stages", redefining names
\vfill
$$\text{Stage}\;1:\;T_{it} = \pi_0 + \pi_1 Z_{it} + \nu_{it}$$
$$\text{Stage}\;2:\;Y_{it} = \phi_0 + \phi_1 T_{it} + \zeta_{it}$$
\vfill

- Requirements:
  - $cov(Z_{it}, T_{it}) \neq 0$
  - $cov(Z_{it}, \zeta_{it}) = 0$
\vfill
- We first regress T on the instrument to get $\hat{T}_{it}$
- Then, we use the predicted values of T to estimate the effects on Y
  - If the IV is valid, these predicted values *are unrelated to the omitted variables!*
\vfill










## Some comments
$$\text{Stage}\;1:\;T_{it} = \pi_0 + \pi_1 Z_{it} + \nu_{it}$$

\vfill
\begin{gather}cov(Z_{it}, T_{it}) \neq 0\end{gather}

- This is the first requirement
\vfill
- We can test this!
  - F-test of all *excluded instruments* in the first stages
  - I say all excluded instruments because you can technically have more than one
\vfill










## Some comments
$$\text{Stage}\;1:\;T_{it} = \pi_0 + \pi_1 Z_{it} + \nu_{it}$$
$$\text{Stage}\;2:\;Y_{it} = \phi_0 + \phi_1 T_{it} + \zeta_{it}$$

\begin{gather}cov(Z_{it}, \zeta_{it}) = 0\end{gather}

- This is the second requirement
\vfill
- We cannot explicitly test this
  - This is an identifying *assumption*
  - We need this to be true to attribute causality to the second stage
\vfill










## Some comments
$$\text{Stage}\;1:\;T_{it} = \pi_0 + \pi_1 Z_{it} + \nu_{it}$$
$$\text{Stage}\;2:\;Y_{it} = \phi_0 + \phi_1 T_{it} + \zeta_{it}$$

\begin{gather}cov(Z_{it}, \zeta_{it}) = 0\end{gather}

- Note that we will use $Z_{it}$ to predict $T_{it}$.
  - We cannot actually observe $cov(Z_{it}, \zeta_{it})$
\vfill
- So if $cov(Z_{it}, \zeta_{it})\neq0$...
  - Then this correlation will be contained in the predicted values, $\hat{T}_{it}$
  - i.e. the predicted values will still be endogenous
\vfill










## IVs in supply and demand
\vfill
- Economists have long been interested in supply and demand
  - Obviously...
\vfill
- How does a change in supply affect prices?
  - Not a straightforward question to answer, because prices are determined jointly by supply and demand
  - We can't determine what is changing when we observe market prices
  - One option: an instrument that moves only one side of the market
\vfill
- Small note: this is how IVs originally came about in economics
\vfill










## Favara and Imbs, 2015 (*American Economic Review*)
\vfill
- How does the availability of credit affect house prices?
\vfill
- They use a change in deregulation of banks in the US
  - This deregulation led to an increase in credit supply
  - But it did not affect credit demand, since it was a supply-side change
\vfill
- Idea: show the change in credit availability for banks affected by the change
  - And no change for banks not affected by the change
\vfill










## Deregulation index across states and years
\center
\includegraphics[width = 0.9\textwidth]{assets/deregulation1.png}










## Two stages: predict credit supply, then predict house prices
\begin{align} &\text{Stage 1: } credit_{ct} = \delta_0 + \delta_1 deregulation_{ct} + \delta_2 X_{ct} + \alpha_c + \gamma_t + \nu_{ct} \\
              &\text{Stage 2: } price_{ct} = \beta_0 + \beta_1 credit_{ct} + \beta_2 X_{ct} + \phi_c + \eta_t + \zeta_{ct} \end{align}

- They instrument for $credit$ using $deregulation$
  - $deregulation$ is correlated with $credit$ but not with $\zeta_{ct}$, according to the authors
  - (Let's ignore whether this is true for now since it's so contextual)
\vfill
- They control for $X_{ct}$, which is a vector of controls
- This is also a two-way fixed effects specification:
  - $\alpha_c$ and $\gamma_t$ ($\phi_c$ and $\eta_t$ in stage 2) are county and year fixed effects
\vfill










## Replication data: `hmda_merged.dta`
```{r rep1, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
library(haven)
df <- read_dta("hmda_merged.dta")
head(df)

# key controls: LDl_hpi Dl_inc LDl_inc Dl_pop LDl_pop Dl_her_v LDl_her_v
# instrument: Linter_bra
# endogenous variables: Dl_nloans_b Dl_vloans_b Dl_lir_b
# weights: w1
# restriction: border counties only (border==1)
# county and year FE
# cluster on state
```










## Reduced form
\vfill
- It is common to estimate the reduced form of the first stage
  - This is a regression of the outcome of interest on the instrument
\vfill
- In this case, this equals
\begin{gather} price_{ct} = B_0 + B_1 deregulation_{ct} + B_2 X_{ct} + \cdots \end{gather}
\vfill









## Reduced form
```{r rep1b, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
bordercounties <- df %>% filter(border==1)
summary(feols(Dl_hpi ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
        data = bordercounties, weights = bordercounties$w1,
        cluster = "state_n"))
```









## First stage
```{r rep2, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
bordercounties <- df %>% filter(border==1)
reg1 <- feols(Dl_nloans_b ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
              data = bordercounties, weights = bordercounties$w1,
              cluster = "state_n")
reg2 <- feols(Dl_vloans_b ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
              data = bordercounties, weights = bordercounties$w1,
              cluster = "state_n")
reg3 <- feols(Dl_lir_b ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
              data = bordercounties, weights = bordercounties$w1,
              cluster = "state_n")
```










## First stage
```{r rep3, echo = FALSE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
bordercounties <- df %>% filter(border==1)
reg1 <- feols(Dl_nloans_b ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
              data = bordercounties, weights = bordercounties$w1,
              cluster = "state_n")
reg2 <- feols(Dl_vloans_b ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
              data = bordercounties, weights = bordercounties$w1,
              cluster = "state_n")
reg3 <- feols(Dl_lir_b ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
              data = bordercounties, weights = bordercounties$w1,
              cluster = "state_n")
table <- etable(reg1, reg2, reg3,
                digits = 3, fitstat = c("n"), se.below = TRUE, depvar = FALSE,
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1))
colnames(table) <- c("", "Loans", "Loan volume", "Loan-to-inc. ratio")
table[c(1,3,5,7,9,11,13,15),1] <- c("IV", "House price (lag)", "Inc. p.c.", "Inc. p.c. (lag)", "Population", "Population (lag)", "Herf. index", "Herf. index (lag)")
table <- table[-c(17:21),]
tabletemp <- etable(reg1, reg2, reg3,
                digits = 10, fitstat = c("n"), se.below = TRUE, depvar = FALSE,
                coefstat = "tstat",
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1))
# extract t statistics for first coeffcicient only
tstats <- as_vector(tabletemp[2,2:4])
# remove parentheses
tstats <- gsub("\\(", "", tstats)
tstats <- gsub("\\)", "", tstats)
tstats <- as.numeric(tstats)
# square for F-test
tstats <- tstats^2
# round to three digits
tstats <- round(tstats, 3)
# add to bottom of table
table[18,] <- c("F-test for instrument", tstats)
kable(table, 
      align = "lccc", booktabs = TRUE, linesep = "", escape = FALSE, row.names = FALSE) %>%
      row_spec(16, hline_after = TRUE) %>%
      column_spec(1,width = "3cm") %>%
      column_spec(c(2:4),width = "2.5cm") %>%
      kable_styling() %>%
      footnote("Note: F-test differs from results in paper due to differences in how xtreg calculates standard errors.", general_title = "",
                footnote_as_chunk = TRUE,
                escape = FALSE
                ) %>%
      footnote("Standard errors clustered on state in parentheses.", general_title = "",
                footnote_as_chunk = TRUE,
                escape = FALSE
                )
```










## First stage predictions vs. actual values... what do you notice?
```{r rep4, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE, size = "tiny", out.width = "55%", fig.align = "center"}
bordercounties <- df %>% filter(border==1)
reg1 <- feols(Dl_nloans_b ~ Linter_bra + LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v | county + year, 
              data = bordercounties, weights = bordercounties$w1,
              cluster = "state_n")
# predict out of first stage
bordercounties$credit <- NA
bordercounties$credit[reg1$obs_selection$obsRemoved] <- predict(reg1)
bordercounties$credit2 <- NA
bordercounties$credit2[reg2$obs_selection$obsRemoved] <- predict(reg2)
bordercounties$credit3 <- NA
bordercounties$credit3[reg3$obs_selection$obsRemoved] <- predict(reg3)

# create plot with predicted vs. actual
ggplot(bordercounties, aes(x = Dl_nloans_b, y = credit)) +
  geom_point(alpha = 0.5) +
  labs(x = "Actual change", y = "Predicted change") +
  # same limits on both axes
  coord_cartesian(xlim = c(min(bordercounties$Dl_nloans_b, na.rm = T), max(bordercounties$Dl_nloans_b, na.rm = T)), 
                  ylim = c(min(bordercounties$Dl_nloans_b, na.rm = T), max(bordercounties$Dl_nloans_b, na.rm = T))) +
  theme_minimal()

```










## First stage predictions vs. actual values... what do you notice?
\vfill
```{r rep5, echo = FALSE, eval = TRUE, message = FALSE, warning = FALSE}
sums <- bordercounties %>% 
          dplyr::select(Dl_nloans_b, credit) %>% 
          na.omit() %>%
          summarize(across(everything(), list(min = min, max = max, sd = sd)))
sums <- rbind(sums, sums)
sums[2,1:3] <- sums[2,4:6]
sums <- sums[,1:3]
sums <- cbind(sums[,1], sums)
sums[,1] <- c("Actual", "Predicted")
colnames(sums) <- c("", "min", "max", "SD")
sums[,2:4] <- round(sums[,2:4], 3)
kable(sums,
      align = "lccc", booktabs = TRUE, linesep = "", escape = FALSE, row.names = FALSE
      ) %>%
      kable_styling()
```

\vfill
- Note how much less variance there is in the predicted values than the actual values
  - This is the point of using an instrument!
  - We are able to isolate the variation in the endogenous variable that is not correlated with the error term
    - This is of course only a subset of the total variation in the endogenous variable
\vfill
- This will be important later
\vfill










## We cannot simply use the predicted values in the second stage... standard errors will be wrong!
```{r rep6, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
# create a macro for the main regression controls (to avoid repetition and save space)
setFixest_fml(..controls = ~ LDl_hpi + Dl_inc + LDl_inc + Dl_pop + LDl_pop + Dl_her_v + LDl_her_v)
# Let's use feols to estimate the two stages
reg1 <- feols(Dl_hpi ~ ..controls | county + year | Dl_nloans_b ~ Linter_bra, 
              data = df, weights = df$w1,
              cluster = "state_n")
reg2 <- feols(Dl_hpi ~ ..controls | county + year | Dl_vloans_b ~ Linter_bra, 
              data = df, weights = df$w1,
              cluster = "state_n")
reg3 <- feols(Dl_hpi ~ ..controls | county + year | Dl_lir_b ~ Linter_bra, 
              data = df, weights = df$w1,
              cluster = "state_n")
```










## `fixest` will give us the correct standard errors, however (first stage)
```{r rep7, echo = TRUE, eval = FALSE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
# first stage:
etable(
      reg1, reg2, reg3,
      stage = 1,
      se.below = TRUE,
      depvar = FALSE,
      signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
      digits = "r3",
      digits.stats = "r0",
      fitstat = c("ivwald", "n"), # make sure to use ivwald for first-stage F-test
      coefstat = "se",
      group = list(controls = "LDl_hpi"),
      keep = "Linter_bra"
    )
```










## `fixest` will give us the correct standard errors, however (first stage)
```{r rep8, echo = FALSE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
# first stage:
table <- etable(
              reg1, reg2, reg3,
              stage = 1,
              se.below = TRUE,
              depvar = FALSE,
              signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
              digits = "r3",
              digits.stats = "r3",
              fitstat = c("ivwald", "n"), # make sure to use ivwald for first-stage F-test
              coefstat = "se",
              group = list(controls = "LDl_hpi"),
              keep = "Linter_bra"
            )
table[1,1] <- "IV (deregulation index)"
colnames(table) <- c("", "Loans", "Loan volume", "Loan-to-inc. ratio")
table[4,2:4] <- ""
table <- table[-c(7:8),]
kable(table, 
      align = "lccc", booktabs = TRUE, linesep = "", escape = FALSE, row.names = FALSE) %>%
      row_spec(6, hline_after = TRUE) %>%
      column_spec(1,width = "3cm") %>%
      column_spec(c(2:4),width = "2.5cm") %>%
      kable_styling() %>%
      footnote("Note: The Wald (similar to F-test) values do not equal the values in the paper due to differences in how xtreg calculates standard errors.", general_title = "",
                footnote_as_chunk = TRUE,
                escape = FALSE
                ) %>%
      footnote("Standard errors clustered on state in parentheses.", general_title = "",
                footnote_as_chunk = TRUE,
                escape = FALSE
                )
```










## `fixest` will give us the correct standard errors, however (second stage)
```{r rep9, echo = TRUE, eval = FALSE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
# second stage:
etable(
      reg1, reg2, reg3,
      stage = 2,
      se.below = TRUE,
      depvar = FALSE,
      signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
      digits = "r3",
      digits.stats = "r3",
      fitstat = c("ivwald", "n"), # make sure to use ivwald for first-stage F-test
      coefstat = "se",
      group = list(controls = "LDl_hpi"),
      keep = c("Dl_nloans_b", "Dl_vloans_b", "Dl_lir_b")
    )
```










## `fixest` will give us the correct standard errors, however (second stage)
```{r rep10, echo = FALSE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
# first stage:
table <- etable(
              reg1, reg2, reg3,
              stage = 2,
              se.below = TRUE,
              depvar = FALSE,
              signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
              digits = "r3",
              digits.stats = "r3",
              fitstat = c("ivwald", "n"),
              coefstat = "se",
              group = list(controls = "LDl_hpi"),
              keep = c("Dl_nloans_b", "Dl_vloans_b", "Dl_lir_b")
            )
table[c(1, 3, 5),1] <- c("Loans", "Loan volume", "Loan-to-inc. ratio")
colnames(table) <- c("", "(1)", "(2)", "(3)")
table[8,2:4] <- ""
table <- table[-c(11:12),]
table[11,3] <- table[12,3]
table[11,4] <- table[13,4]
table <- table[-c(12:13),]
table[11,1] <- "Wald (1st stage)"
kable(table, 
      align = "lccc", booktabs = TRUE, linesep = "", escape = FALSE, row.names = FALSE) %>%
      row_spec(10, hline_after = TRUE) %>%
      column_spec(1,width = "3cm") %>%
      column_spec(c(2:4),width = "2.5cm") %>%
      kable_styling() %>%
      footnote("Standard errors clustered on state in parentheses. The Wald (similar to F-test) values do not equal the values in the paper due to differences in how xtreg calculates standard errors. The standard errors here are more conservative.", 
                general_title = "",
                threeparttable = TRUE,
                footnote_as_chunk = TRUE,
                escape = FALSE
                )
```










## Note the syntax for `fixest`
\vfill
`feols(y ~ x | fe1 + fe2 | endogenousvar ~ z, ...)`
\vfill
`feols(y ~ x | fe1 + fe2 | endogenousvar1 + endogenousvar2 ~ z1 + z2, ...)`
\vfill

- All controls should be in the first stage, as well as the second
  - `fixest` does this for us automatically

- The package also automatically calculates correct standard errors in the second stage
  - For the "generated regressor"
\vfill










## Estimating it all together
\vfill
- With just a single instrument and a single endogenous variable, there is a single first stage
\vfill
- Let's continue with our outcome $Y$, our endogenous variable $X$, and our exogenous variables $Z$ (which includes the instrument)
\vfill
- It turns out that we can write $\hat{\beta}_{IV}$ as:
\begin{gather} \hat{\beta}_{IV}=\left((Z'Z)^{-1}(Z'X)\right)^{-1}\left((Z'Z)^{-1}(Z'Y)\right) \end{gather}
\vfill







## Estimating it all together
\vfill
\begin{gather} \tag{14} \hat{\beta}_{IV}=\left((Z'Z)^{-1}(Z'X)\right)^{-1}\left((Z'Z)^{-1}(Z'Y)\right) \end{gather}

- We can immediately see two things:
\vfill
  - The requirement that $Z$ predicts $X$ is necessary to invert the first term
\vfill
  - The IV estimate *scales the reduced form by the first stage*
\vfill







## Just a quick note that this simplifies
\begin{align} \tag{14} \hat{\beta}_{IV}&=\left((Z'Z)^{-1}(Z'X)\right)^{-1}\left((Z'Z)^{-1}(Z'Y)\right) \\
                                        &=(Z'X)^{-1}(Z'Z)(Z'Z)^{-1}(Z'Y) \\
                                        &=(Z'X)^{-1}(Z'Y) \end{align}








## Binary instrument and binary treatment
\vfill
- Let's consider a binary instrument and a binary treatment
  - $Z$ and $D$ are binary, i.e. $Z,D\in\{0,1\}$
\vfill
- It turns out there is a very real case where we can find a valid instrument that is binary
  - Treatment assignment in an RCT!
\vfill








## RCTs and IV
\vfill
- Banerjee et al. (2015): The Miracle of Microfinance? Evidence from a Randomized Evaluation (*AEJ: Applied*)
\vfill
- They are interested in the effects of access to credit on outcomes
  - They randomly assign households (sort of) to microcredit *access*
\vfill
- Z: whether or not the household was offered microcredit
  - This is a binary instrument
- X: whether or not the household received credit
  - This is a binary endogenous variable
\vfill








## RCTs and IV
\vfill
- Banerjee et al. (2015): The Miracle of Microfinance? Evidence from a Randomized Evaluation (*AEJ: Applied*)
\vfill
- They are interested in the effects of access to credit on outcomes
  - They randomly assign households (sort of) to microcredit *access*
\vfill
- Z: whether or not the household was offered microcredit
  - This is a binary instrument
- X: whether or not the household received credit
  - This is a binary endogenous variable
\vfill








## Effects of the program on outcomes in endline 1
```{r micro1, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
df <- read_dta("banerjeeetal.dta")
# create a macro for the main regression controls (to avoid repetition and save space)
setFixest_fml(..controls = ~ area_pop_base + area_debt_total_base + area_business_total_base + area_exp_pc_mean_base + 
                              area_literate_head_base + area_literate_base)
# they control for baseline values of NEIGHBORHOOD means of these variables
```
```{r microb, echo = FALSE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
df <- df %>% dplyr::select(areaid, treatment, w1, starts_with("area_"), anymfi_1, anyloan_1, bizassets_1, bizprofit_1, any_biz_1)
# make sure we have same sample
df <- df[complete.cases(df),]
```


- They estimate:
\begin{gather} y_{in} = \beta_0 + \beta_1 Z_{n} + \sum_{k=1}^K\gamma_k X_k + \varepsilon_{n}, \end{gather}
where $Z_{i}$ is the treatment variable (microcredit access) and standard errors are clustered at the areaid (neighborhood)








## Reduced form
```{r micro2, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
reg1 <- feols(any_biz_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg2 <- feols(bizassets_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg3 <- feols(bizprofit_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
table <- etable(reg1, reg2, reg3,
                digits = 3, fitstat = c("n"), se.below = TRUE, depvar = FALSE,
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
                group = list(controls = "area_pop_base"), keep = "treatment")
```








## Reduced form, clean table
```{r micro3, echo = FALSE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
reg1 <- feols(any_biz_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg2 <- feols(bizassets_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg3 <- feols(bizprofit_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
table <- etable(reg1, reg2, reg3,
                digits = 3, fitstat = c("n"), se.below = TRUE, depvar = FALSE,
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
                group = list(controls = "area_pop_base"), keep = "treatment")
colnames(table) <- c("", "Any biz?", "Biz assets", "Biz profits")
table <- table[-c(4:5),]
kable(table, 
      align = "lccc", booktabs = TRUE, linesep = "", escape = FALSE, row.names = FALSE) %>%
      row_spec(3, hline_after = TRUE) %>%
      column_spec(1,width = "2cm") %>%
      column_spec(c(2:4),width = "1.5cm") %>%
      kable_styling() %>%
      footnote("Standard errors clustered on neighborhood in parentheses.", general_title = "",
                footnote_as_chunk = TRUE,
                escape = FALSE
                )
```








## First stage
```{r micro4, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
reg1 <- feols(anymfi_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg2 <- feols(anyloan_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
table <- etable(reg1, reg2,
                digits = 3, fitstat = c("n"), se.below = TRUE, depvar = FALSE,
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
                group = list(controls = "area_pop_base"), keep = "treatment")
```








## First stage, clean table
```{r micro5, echo = FALSE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
reg1 <- feols(anymfi_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg2 <- feols(anyloan_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
table <- etable(reg1, reg2,
                digits = 3, fitstat = c("n"), se.below = TRUE, depvar = FALSE,
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
                group = list(controls = "area_pop_base"), keep = "treatment")
colnames(table) <- c("", "Any MFI loan?", "Any loan?")
table <- table[-c(4:5),]
kable(table, 
      align = "lcc", booktabs = TRUE, linesep = "", escape = FALSE, row.names = FALSE) %>%
      row_spec(3, hline_after = TRUE) %>%
      column_spec(1,width = "2cm") %>%
      column_spec(c(2:3),width = "1.5cm") %>%
      kable_styling() %>%
      footnote("Standard errors clustered on neighborhood in parentheses.", general_title = "",
                footnote_as_chunk = TRUE,
                escape = FALSE
                )
```








## IV results
```{r micro6, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
reg1 <- feols(any_biz_1 ~ ..controls | anymfi_1 ~ treatment, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg2 <- feols(bizassets_1 ~ ..controls | anymfi_1 ~ treatment, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg3 <- feols(bizprofit_1 ~ ..controls | anymfi_1 ~ treatment, 
              data = df, weights = df$w1,
              cluster = "areaid")

table <- etable(reg1, reg2, reg3,
                digits = 3, fitstat = c("ivwald", "n"), se.below = TRUE, depvar = FALSE,
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
                group = list(controls = "area_pop_base"), keep = "anymfi_1")
```








## IV results, clean table
```{r micro7, echo = FALSE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
reg1 <- feols(any_biz_1 ~ ..controls | anymfi_1 ~ treatment, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg2 <- feols(bizassets_1 ~ ..controls | anymfi_1 ~ treatment, 
              data = df, weights = df$w1,
              cluster = "areaid")
reg3 <- feols(bizprofit_1 ~ ..controls | anymfi_1 ~ treatment, 
              data = df, weights = df$w1,
              cluster = "areaid")

table <- etable(reg1, reg2, reg3,
                digits = 3, fitstat = c("ivwald", "n"), se.below = TRUE, depvar = FALSE,
                # change significance codes to the norm
                signif.code = c("***" = 0.01, "**" = 0.05, "*" = 0.1),
                group = list(controls = "area_pop_base"), keep = "anymfi_1")
colnames(table) <- c("", "Any biz?", "Biz assets", "Biz profits")
table <- table[-c(4:5),]
table[1,1] <- "Has MFI loan"
table[4,1] <- "Wald (1st stage)"
kable(table, 
      align = "lccc", booktabs = TRUE, linesep = "", escape = FALSE, row.names = FALSE) %>%
      row_spec(3, hline_after = TRUE) %>%
      column_spec(1,width = "2cm") %>%
      column_spec(c(2:4),width = "1.5cm") %>%
      kable_styling() %>%
      footnote("Standard errors clustered on neighborhood in parentheses.", general_title = "",
                footnote_as_chunk = TRUE,
                escape = FALSE
                )
```








## Putting them together
```{r micro8, echo = TRUE, eval = TRUE, message = FALSE, warning = TRUE, size = "tiny", out.width = "55%", fig.align = "center"}
# reduced form
reg1 <- feols(any_biz_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
# first stage
reg2 <- feols(anymfi_1 ~ treatment + ..controls, 
              data = df, weights = df$w1,
              cluster = "areaid")
# IV result
reg3 <- feols(any_biz_1 ~ ..controls | anymfi_1 ~ treatment, 
              data = df, weights = df$w1,
              cluster = "areaid")
```

- Coefficient on reduced form: `r paste0(reg1$coefficients[2])`
\vfill
- Coefficient on first stage: `r paste0(reg2$coefficients[2])`
\vfill
- Coefficient on IV: `r paste0(reg3$coefficients[2])`
  - Can you figure out how this is related to the RF and FS? \pause
  - This is a ratio: $\frac{\hat{\beta}_{RF}}{\hat{\beta}_{FS}} = \hat{\beta}_{IV}$
  - The IV result *scales the reduced form by the first stage*
\vfill








## Putting them together, the intuition
\vfill
- The IV estimate is a ratio of two coefficients
  - The reduced form coefficient and the first stage coefficient
\vfill
- In this example, treatment increases MFI loan take-up by 8.2 percentage points.
  - In other words, the treatment effect is driven by a change in MFI loan take-up among 8.2 percent of households
\vfill
- If the probability of owning a business goes up by 0.005 (0.5 p.p.), what is the change in probability of owning a business for those who take up the MFI loan?
  - 0.005/0.082! This is the IV estimate
\vfill








## The Wald estimator
\vfill
- This is sometimes referred to as the wald estimator (Wald 1940)
\begin{gather} \beta = \frac{\mathbb{E}\left[Y\mid Z=1\right]-\mathbb{E}\left[Y\mid Z=0\right]}{\mathbb{E}\left[X\mid Z=1\right]-\mathbb{E}\left[X\mid Z=0\right]} \end{gather}

- Note that these expectations are not observed
  - We estimate them with the reduced form and first stage
\vfill















## Interpreting IV estimates
\vfill
- So this IV estimate is driven by the change in MFI loan take-up among 8.2 percent of households
  - What does this mean for the effect of MFI loans on business ownership?
\vfill
- Two worlds:
  - Homogeneous treatment effects
  - Heterogeneous treatment effects
\vfill
- Remember how I said an IV identifies just certain kinds of variation?
  - This will come into play here
\vfill










## Homogeneous treatment effects
\vfill
- We had a similar discussion when we talked about DiD
\vfill
- If everyone has the same treatment effect, then it doesn't matter what variation we isolate
  - All variation will be identifying the same effect
\vfill
- In this case, the IV is estimated the average treatment effect
\vfill
- But what if effects are not homogeneous?
\vfill









## Heterogeneous treatment effects
\vfill
- What if not everyone has the same treatment effect?
  - In other words, what if different types of variation are identifying different effects?
\vfill
- Imagine a world in which we have an endogenous variable, $D$
  - Imagine we also have multiple *valid* instruments: $Z_1$ and $Z_2$
\vfill
- If $Z_1$ and $Z_2$ are correlated with different "parts" of $D$, then they can be isolating different variation in $D$
  - This also means that they IV results can lead to different estimates, even though both instruments are valid!
\vfill









## Defining the LATE
\vfill
- We need to define four separate groups:
  - Compliers
  - Always-takers
  - Never-takers
  - Defiers
\vfill
- Let's look at these four groups assuming a binary treatment
\vfill









## Compliers
\center
\includegraphics[width = 0.85\textwidth]{assets/late1.png}









## Never-takers
\center
\includegraphics[width = 0.85\textwidth]{assets/late2.png}









## Always-takers
\center
\includegraphics[width = 0.85\textwidth]{assets/late3.png}









## Defiers
\center
\includegraphics[width = 0.85\textwidth]{assets/late4.png}









## In Hansen, where X is treatment assignment
\center
\includegraphics[width = 0.85\textwidth]{assets/hansenlate.png}









## Comparing the four groups
\center
\includegraphics[width = 0.55\textwidth]{assets/late5.png}









## What are we estimating?
\vfill
- Never takers *never* take up the treatment
  - If we have no variation in treatment for them, we can't estimate the effect of the treatment on them
  - Same goes for always takers
\vfill
- That leaves us with two groups: compliers and defiers
  - Let's make one more assumption: $P(X(1)-X(0)<0)=0$ (or $>0$)
  - i.e. there are no defiers
\vfill









## What are we estimating?
\vfill
- This is called the local average treatment effect (LATE)
\vfill
- This is the *effect of the treatment on compliers*
  - i.e. the effect of the treatment on those who are induced to take up treatment because of the instrument
\vfill
- Again, if treatment is homogeneous, the effect on compliers is the same on others
  - In this case, the LATE is the ATE
  - But, do we really think this is ever true?
\vfill









## Different instruments, different effects
\vfill
- One implication of LATE is that different instruments can identify different effects
  - In other words, the group of "compliers" can differ across instruments, even if all the instruments are valid
\vfill
- Example:
  - Interested in the effects of going to college
  - Instrument 1: whether or not you live close to a college
  - Instrument 2: whether or not you have a scholarship
\vfill









## This might be okay, though
\vfill
- When we think about interventions, we often think about the *margins* of the intervention
  - In other words, we are interested in the effect of the intervention on those who are induced to take up the intervention
\vfill
- If a government is considering a new program/policy, then the effects will always be driven by those who are induced to take up the program/policy
  - In other words, the compliers
  - So identifying a LATE might actually be policy relevant in some contexts!
\vfill
- One final note:
  - The LATE interpretation also holds for non-binary instruments
  - Interpretation of what it means to be a "complier" is a bit more complicated, though
\vfill









## Some notes on compliers under LATE
\vfill
- The first stage tells us the complier share of the overall population (if it's binary)
  - A small note: the more compliers there are, the less problematic violations of the exclusion restriction are (Angrist et al., 1996)
\vfill
- We can learn a bit about characteristics of compliers, too, using a similar intuition
  - Works with discrete characteristics
\vfill









## Weak instruments
\vfill
- Let's return to our discussion about the first stage: $Z$ must be correlated with $X$
  - If $Z$ is not correlated with $X$, then we cannot identify the effect of $X$ on $Y$
\vfill
- We often think about this in terms of the first stage F-statistic
  - Is the F-statistic is high "enough"?
  - What is high "enough" in this context?]
\vfill
- We used to think about $F>10$, but recent literature argues it should be even higher!
  - e.g. Plfueger and Wang (2013) closer to 23
  - Lee et al. (2020) argue for 100 or higher
    - Focus on t-statistic, not the coefficient
    - Lower F-statistics mean the critical value should actually be higher than 1.96
  - No "right" answer, but higher is better
\vfill









## Compulsory school attendance and earnings
\vfill
- Let's look at an example: Angrist and Krueger (1991)
  - They are interested in the returns to schooling
\vfill
- Basic idea:
  - School attendance laws require students to stay in school until a certain age
  - Consider a school year that starts on August 1st
    - Someone who was born on July 31st will be one year older at the start of the school year than someone born on August 2nd
\vfill
- Instrument for school attendance using the time of birth
  - "Individuals born in the beginning of the year start school at an older age, and can therefore drop out after completing less schooling than individuals born near the end of the year."
\vfill









## Compulsory school attendance and earnings, year/quarter of birth
\center
\includegraphics[width = 0.9\textwidth]{assets/angristkrueger1.png}









## Compulsory school attendance and earnings, reduced form
\center
\includegraphics[width = 0.9\textwidth]{assets/angristkrueger2.png}









## The model
\vfill
\begin{gather} y = \beta s + \varepsilon \\
                s = \gamma Z + \eta, \end{gather}

- $y$ is earnings
- $s$ is years of schooling
- $Z$ is the instrument
  - They  use interactions between year and quarter of birth
\vfill









## Bias in OLS
\vfill
- If $\varepsilon$ and s are correlated, then OLS gives biased estimates
\vfill
- The bias is:
  \begin{gather} E\left[\hat\beta_{OLS}-\beta\right] = \frac{Cov(s,\varepsilon)}{Var(s)} \end{gather}

- Let's rename this ratio as $\frac{\sigma_{\varepsilon\eta}}{\sigma_{s}^2}$
\vfill









## Bias in OLS and first stage F-statistics
\vfill
- It turns out we can approximate the bias in 2SLS as:
\begin{gather}\frac{\sigma_{\varepsilon\eta}}{\sigma_{s}^2}\frac{1}{F+1} \end{gather}

- Note that if the first stage is weak, $F$ is closer to zero and the 2SLS bias is closer to the OLS bias
  - If the first stage is strong, $F$ is larger and the bias gets closer to zero
\vfill









## Bound et al. (1995), *JASA*
\vfill
- Bound et al. (1995) were the first to point this problem out
  - You see, Angrist and Krueger, added a *lot* of instruments to some of their specifications
  - The addition of more instruments can be a problem: it tends to decrease the first-stage F-statistic
\vfill
- Let's take a look at their results
\vfill









## Note what happens to the IV coefficient as F decreases
\center
\includegraphics[width = \textwidth]{assets/bound1.png}









## A weak first stage won't necessarily lead to large standard errors
\vfill
- I used to think a weak first stage would lead to large standard errors
  - This is not necessarily true
\vfill
- Bound et al. do a simulation exercise where they create *completely random instruments*
  - In other words, by construction, the instruments should not predict the endogenous variable
\vfill









## Random instruments and standard errors
\center
\includegraphics[width = 0.75\textwidth]{assets/bound2.png}








## More problems with weak instruments
\begin{gather*} \hat{\beta}_{2SLS} = \frac{Cov(Y, Z)}{Cov(X, Z)} \end{gather*}

- We've seen this before: the IV estimate is a ratio of covariances (or the ratio of the reduced form and the first stage)
\vfill
- With weak instruments, $Cov(X,Z)$ is small
  - This means that small changes in $Cov(Y,Z)$ can lead to large changes in $\hat{\beta}_{2SLS}$
  - Asymptotically, this isn't a problem. But in small samples...
  - We're back to something we've seen before: might need relatively large sample sizes to reliably estimate what you want to estimate!
\vfill
- This is a problem with ratios more generally. Try bootstrapping a ratio where the numerator is small and see what happens.
\vfill









## Example from Goldsmith-Pinkham's slides
\vfill
- Rather than create my own, I'm going to use Paul's example
  - https://github.com/paulgp/applied-methods-phd
\vfill
- Let's look at three things:
  - The behavior of the first stage when the instrument is weak (he calls this Pi hat)
  - The relationship between the first stage and the second stage
  - The behavior of the 2SLS estimator as a whole when the instrument is weak
\vfill









## Marginally significant first stage, simulations
\center
\includegraphics[width = 0.9\textwidth]{assets/weak_iv_tauhat_hist.png}









## Marginally significant first stage, simulations
\center
\includegraphics[width = 0.9\textwidth]{assets/weak_iv_tauhat_betahat.png}









## Marginally significant first stage, simulations
\center
\includegraphics[width = 0.9\textwidth]{assets/weak_iv_betahat_hist.png}









## Marginally significant first stage, simulations
\vfill
- The distribution of $\hat\beta$ is absolutely not normal
  - Asymptotics won't save you here!
\vfill
- Note that this problem can (mostly) disappear when the first stage is strong
  - For example, a larger sample size will lead to better behavior of the estimator
\vfill
- Again, asymptotic approximations -- just like with the CLT and skewed distributions -- won't necessary apply
\vfill









## Takewaways
\vfill
- Looking at the second stage won't *necessarily* tell you if the first stage is weak
\vfill
- Nowadays, it is very common to report the first stage F-statistic
  - You can't write a paper without reporting it
\vfill
- The key idea is that many instruments can increase bias, even if it isn't obvious
  - Part of the problem is related to overfitting, which we'll cover in a few Weeks
  - In fact, Angrist and Kolesar (2023) argue that weak instruments may not be a huge problem in the just-identified (i.e. one instrument) case!
\vfill
- Chernozhukov and Hansen (2008) detail a routine to calculate confidence intervals that are valid regardless of the strength of the first stage (in the just-identified case).
  - Packages in both Stata and R
\vfill










## Overidentification tests
\vfill
- In the previous case, we had many instruments
  - This is called overidentification
\vfill
- With overidentification, it is possible to test the "validity" of the instruments...
  - ... if we are willing to assume at least one of the instruments is valid!
\vfill
- The intuition: different instruments should give us the same result
\vfill









## Overidentification tests
\vfill
- Consider a single endogenous $X$ and two instruments, $Z_1$ and $Z_2$:
\begin{gather} \mathbb{E}\left[Z_1\right]=\mathbb{E}\left[Z_1X\right]\beta \\
                \mathbb{E}\left[Z_2\right]=\mathbb{E}\left[Z_2X\right]\beta \end{gather}

- Assumption for overidentification are saying that $\beta$ solves both equations simultaneously
  - In other words, $\beta$ is the same for both instruments
\vfill
- If one instrument is valid and the other isn't, they should give us different results
  - We can test this!
  - Sometimes referred to as an overidentification test, a Sargan test (or Sargan's J), or a Sargan-Hansen test
\vfill









## Overidentification tests
\vfill
- Consider a single endogenous $X$ and two instruments, $Z_1$ and $Z_2$:
\begin{gather} \tag{23} \mathbb{E}\left[Z_1\right]=\mathbb{E}\left[Z_1X\right]\beta \\
                \tag{24} \mathbb{E}\left[Z_2\right]=\mathbb{E}\left[Z_2X\right]\beta \end{gather}

- But there's a problem...
  - And I already mentioned it. What's the problem?
\vfill\pause
- In a world of LATEs, the instruments can identify different effects
  - So we can't really test the validity of the instruments!
\vfill
- TLDR: overidentification tests are not very useful (my take, anyway)
\vfill





